{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "scrapingenv1",
   "display_name": "scrapingEnv1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 유튜브 api 크롤링 결과 분석기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤링 결과 가져오기\n",
    "\n",
    "f = open('./crawling_result_text/gamst.txt', mode='rt', encoding='utf-8')\n",
    "\n",
    "strings = f.read()\n",
    "\n",
    "f.close()\n",
    "\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규표현식 활용 말뭉치 정리\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "stop_special_char = re.sub(pattern='[가-힣 a-zA-Z\\d\\.\\n]+', repl='', string=strings)\n",
    "stop_special_char_pattern = '[' + '\\\\'.join(set(stop_special_char)) + ']'\n",
    "\n",
    "print(stop_special_char_pattern)\n",
    "\n",
    "new_strings = re.sub(pattern=stop_special_char_pattern, repl='',string=strings)\n",
    "new_strings = re.sub(pattern='\\s{2,}',repl=' ',string=new_strings)\n",
    "new_strings = re.sub(pattern='[\\n]+',repl=' ',string=new_strings)\n",
    "new_strings = re.sub(pattern='[a-zA-Z\\,]+',repl='',string=new_strings)\n",
    "new_strings = re.sub(pattern='[0-9]+', repl='', string=new_strings)\n",
    "new_strings = re.sub(pattern='\\s{2,}',repl=' ',string=new_strings)\n",
    "new_strings = re.sub(pattern='롤', repl='리그오브레전드', string=new_strings)\n",
    "new_strings = re.sub(pattern='아프리카', repl='아프리카티비', string=new_strings)\n",
    "new_strings = re.sub(pattern='아프리카티비티비', repl='아프리카티비', string=new_strings)\n",
    "# new_strings = [ line.strip() for line in new_strings.split('.') if len(line) > 4 ]\n",
    "print(new_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#품사 태깅\n",
    "\n",
    "from konlpy.tag import Hannanum\n",
    "\n",
    "hannanum = Hannanum()\n",
    "\n",
    "print(hannanum.nouns(new_strings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#불용어사전(자체 제작)\n",
    "stopwords = ['라', '중', '이', '때문', '지', '이상', '등', '수', '것', '시작', '부분', '당시', '경우', '이후', '오브', '리그', '편', '위', '정도', '활동', '전', \n",
    "    '한', '도중', '자체', '경기', '방송이','때','녹두','이유','문단을','대부분','번','일','리','두','초','나','문단','문','방송','적','후','단어','게임','유튜브','영상','말','역사','시청자들','본인','이전','시청자','2020년','2018년','2019년','2017년','2016년','업로드','콘텐츠','듯','현재','명','들','사람','1','개','관련','채널','모습','주','거','내','사이','자신','유튜버','컨텐츠','생각'] +['1월','2월','3월','4월','5월','6월','7월','8월','9월','10월','11월','12월'] + ['감스트', '비감', '인직'] + ['진짜'] + ['아', '휴', '아이구', '아이쿠', '아이고','어','나','우리','저희','따라','의해','을','를','에','의','가','으로','로','에게','뿐이다','저','지말고','다','뿐','허','헉','곧','이다','도','부터','모','등','조금','좀','대','종','망','그','데','에','애','제','장']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#워드 클라우드 생성\n",
    "\n",
    "from collections import Counter\n",
    "import random\n",
    "import math\n",
    "\n",
    "from konlpy.tag import Hannanum\n",
    "from konlpy.tag import Twitter\n",
    "import pytagcloud\n",
    "import webbrowser\n",
    "\n",
    "r = lambda: random.randint(0,255)\n",
    "color = lambda: (r(),r(),r())\n",
    "\n",
    "def get_tags(text, ntags=100, multiplier=10):\n",
    "    h = Hannanum()\n",
    "    nouns = h.nouns(text)\n",
    "    unique_nouns = set(nouns)\n",
    "    for word in unique_nouns:\n",
    "        if word in stopwords:\n",
    "            while word in nouns:\n",
    "                nouns.remove(word)\n",
    "\n",
    "    count = Counter(nouns)\n",
    "    print(count)\n",
    "    return [{ 'color': color(), 'tag': n, 'size': int(math.sqrt(c)*multiplier) }\\\n",
    "                for n, c in count.most_common(ntags)]\n",
    "\n",
    "\n",
    "\n",
    "def get_tags_twitter(text, ntags=100, multiplier=5):\n",
    "    t =Twitter()\n",
    "    nouns = t.nouns(text)\n",
    "    unique_nouns = set(nouns)\n",
    "    for word in unique_nouns:\n",
    "        if word in stopwords:\n",
    "            while word in nouns:\n",
    "                nouns.remove(word)\n",
    "\n",
    "    count = Counter(nouns)\n",
    "    return [{ 'color': color(), 'tag': n, 'size': int(c*multiplier) }\\\n",
    "                for n, c in count.most_common(ntags)]\n",
    "\n",
    "\n",
    "def draw_cloud(tags, filename, fontname='Korean', size=(1600, 1200)):\n",
    "    pytagcloud.create_tag_image(tags, filename, fontname=fontname, size=size)\n",
    "    webbrowser.open( os.path.relpath(filename) )\n",
    "\n",
    "\n",
    "tags = get_tags(new_strings)\n",
    "# print(tags)\n",
    "draw_cloud(tags,'./crawling_result_wordcloud/WC_You_gamst.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#파일 하나 열어서 전처리 후 단어 추출 후 다시 문장으로 만드는 함수\n",
    "\n",
    "import re\n",
    "from konlpy.tag import Hannanum\n",
    "\n",
    "def make_clean_sting(filename):\n",
    "    # 파일 읽기\n",
    "    with open(filename, mode='rt', encoding='utf-8') as f:\n",
    "        strings = f.read()\n",
    "    \n",
    "    #전처리\n",
    "    stop_special_char = re.sub(pattern='[가-힣 a-zA-Z\\d\\.\\n]+', repl='', string=strings)\n",
    "    stop_special_char_pattern = '[' + '\\\\'.join(set(stop_special_char)) + ']'\n",
    "\n",
    "    new_strings = re.sub(pattern=stop_special_char_pattern, repl='',string=strings)\n",
    "    new_strings = re.sub(pattern='\\s{2,}',repl=' ',string=new_strings)\n",
    "    new_strings = re.sub(pattern='[\\n]+',repl=' ',string=new_strings)\n",
    "    new_strings = re.sub(pattern='[a-zA-Z\\,]+',repl='',string=new_strings)\n",
    "    new_strings = re.sub(pattern='[0-9]+', repl='', string=new_strings)\n",
    "    new_strings = re.sub(pattern='\\s{2,}',repl=' ',string=new_strings)\n",
    "    new_strings = re.sub(pattern='롤', repl='리그오브레전드', string=new_strings)\n",
    "    new_strings = re.sub(pattern='아프리카', repl='아프리카티비', string=new_strings)\n",
    "    new_strings = re.sub(pattern='아프리카티비티비', repl='아프리카티비', string=new_strings)\n",
    "\n",
    "    # 명사 추출\n",
    "    hannanum = Hannanum()\n",
    "    string2noun = hannanum.nouns(new_strings)\n",
    "    \n",
    "    #다시 합치기\n",
    "    noun2string = \" \".join(string2noun)\n",
    "\n",
    "\n",
    "    print('task [%s] clear' % filename)\n",
    "    return noun2string\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc set 만들기\n",
    "\n",
    "gamst = make_clean_sting('./crawling_result_text/gamst.txt')\n",
    "handongsuk = make_clean_sting('./crawling_result_text/handongsuk.txt')\n",
    "testerhoon = make_clean_sting('./crawling_result_text/testerhoon.txt')\n",
    "\n",
    "doc_list = []\n",
    "doc_list.append(gamst)\n",
    "doc_list.append(handongsuk)\n",
    "doc_list.append(testerhoon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "감스트\n아프리카티비\n진짜\n오세블리\n인직\n김인직\n감튜브\n비감\n방송영상\n방송\n\n\n동수칸\n진짜\n버튼\n트위치\n한동숙\n구독\n펨플릭스\n댓글\n카페\n방송국\n\n\n테스터훈\n장인초대석\n바로가\n진짜\n장인\n페이스북\n게임\n코리안메타\n뉴메타\n비즈니스\n"
     ]
    }
   ],
   "source": [
    "# tf-idf \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "x = tfidf.fit_transform(doc_list)\n",
    "word_list = tfidf.get_feature_names()\n",
    "# print(x)\n",
    "# print(x.data)\n",
    "# print(x.indices)\n",
    "# print(x.indptr)\n",
    "\n",
    "\n",
    "new_data_list = []\n",
    "\n",
    "for i in range(0,x.indptr[3]):\n",
    "    new_data = [0,0]\n",
    "    new_data[0] = x.data[i]\n",
    "    new_data[1] = x.indices[i]\n",
    "    new_data_list.append(new_data)\n",
    "\n",
    "top10_list = [[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0]]\n",
    "top10_num_list = [0,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "\n",
    "#첫번째 문서 탑10\n",
    "for i in range(0,x.indptr[1]):\n",
    "    current_value = new_data_list[i][0]\n",
    "    if top10_list[9][0] < current_value:\n",
    "        top10_list[9] = new_data_list[i]\n",
    "        top10_list.sort(key=lambda x:x[0],reverse=True)\n",
    "        \n",
    "\n",
    "for i in top10_list:\n",
    "    print(word_list[i[1]])\n",
    "\n",
    "top10_list = [[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0]]\n",
    "\n",
    "print('\\n')\n",
    "#두번째 문서 탑10\n",
    "for i in range(x.indptr[1],x.indptr[2]):\n",
    "    current_value = new_data_list[i][0]\n",
    "    if top10_list[9][0] < current_value:\n",
    "        top10_list[9] = new_data_list[i]\n",
    "        top10_list.sort(key=lambda x:x[0],reverse=True)\n",
    "        \n",
    "\n",
    "for i in top10_list:\n",
    "    print(word_list[i[1]])\n",
    "\n",
    "top10_list = [[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0]]\n",
    "\n",
    "print('\\n')\n",
    "#세번째 문서 탑10\n",
    "for i in range(x.indptr[2],x.indptr[3]):\n",
    "    current_value = new_data_list[i][0]\n",
    "    if top10_list[9][0] < current_value:\n",
    "        top10_list[9] = new_data_list[i]\n",
    "        top10_list.sort(key=lambda x:x[0],reverse=True)\n",
    "        \n",
    "\n",
    "for i in top10_list:\n",
    "    print(word_list[i[1]])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ]
}